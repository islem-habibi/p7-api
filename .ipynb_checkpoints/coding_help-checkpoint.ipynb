{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ace34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install genanki\n",
    "\n",
    "#pip install \"python-dotenv[cli]\"\n",
    "\n",
    "\n",
    "#pip install openai==0.28 -q\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] =\"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\"\n",
    "#os.getenv('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')\n",
    "\n",
    "#def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "def get_completion(prompt, model=\"gpt-4-1106-preview\"): #gpt-4\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837960b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def lire_contenu_site(url):\n",
    "    # Effectuer une requête pour obtenir le contenu de la page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Vérifier si la requête a réussi (code de statut 200)\n",
    "    if response.status_code == 200:\n",
    "        # Utiliser BeautifulSoup pour analyser le contenu HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extraire et afficher le contenu que vous souhaitez\n",
    "        # Par exemple, pour extraire le texte à partir de balises <p>:\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            print(paragraph.get_text())\n",
    "    else:\n",
    "        # Afficher un message d'erreur si la requête a échoué\n",
    "        print(f\"Erreur {response.status_code}: Impossible d'obtenir le contenu de la page.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976f4c6",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a270ebb",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18454a93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre: Analyse et Traitement de Données E-commerce avec Techniques de Machine Learning\n",
      "\n",
      "Introduction:\n",
      "Ce rapport présente une analyse détaillée et le traitement de données issues d'un échantillon de produits e-commerce. Nous avons utilisé des techniques de traitement de texte et d'image pour extraire des caractéristiques et appliquer des modèles de machine learning pour la classification des produits.\n",
      "\n",
      "Partie 1: Observation et Exploration des Données\n",
      "\n",
      "1. Chargement des données:\n",
      "   - Les données sont chargées à partir d'un fichier CSV stocké sur Google Drive.\n",
      "   - Le DataFrame `df` est créé à partir de ces données.\n",
      "\n",
      "2. Exploration initiale:\n",
      "   - Utilisation de `df.info()` pour obtenir un aperçu des types de données et des valeurs manquantes.\n",
      "   - La colonne `product_category_tree` est examinée pour comprendre la hiérarchie des catégories.\n",
      "\n",
      "3. Traitement des catégories:\n",
      "   - La colonne `product_category_tree` est divisée en trois nouvelles colonnes (`categorie 1`, `categorie 2`, `categorie 3`) pour représenter les catégories hiérarchiques des produits.\n",
      "\n",
      "Partie 2: Analyse Textuelle\n",
      "\n",
      "1. Prétraitement du texte:\n",
      "   - Suppression des stopwords et ponctuation.\n",
      "   - Lemmatisation des mots pour réduire les mots à leur forme de base.\n",
      "\n",
      "2. Vectorisation du texte:\n",
      "   - Application de TF-IDF pour convertir le texte en une matrice de caractéristiques numériques.\n",
      "   - Utilisation de Bag of Words (BOW) et Word2Vec pour des approches alternatives d'extraction de caractéristiques.\n",
      "\n",
      "3. Embedding de phrases:\n",
      "   - Utilisation de BERT et Universal Sentence Encoder (USE) pour obtenir des représentations vectorielles des descriptions de produits.\n",
      "\n",
      "Partie 3: Réduction de Dimension et Visualisation\n",
      "\n",
      "1. Application de NMF:\n",
      "   - Réduction de dimension avec NMF pour regrouper les produits en fonction de leurs descriptions.\n",
      "   - Visualisation avec t-SNE pour observer la séparation des clusters.\n",
      "\n",
      "2. Application de LDA:\n",
      "   - Utilisation de Latent Dirichlet Allocation pour identifier les sujets dans les descriptions de produits.\n",
      "\n",
      "Partie 4: Classification Supervisée\n",
      "\n",
      "1. Préparation des données:\n",
      "   - Encodage des étiquettes de catégorie avec LabelEncoder.\n",
      "   - Séparation des données en ensembles d'entraînement et de test.\n",
      "\n",
      "2. Modèles de classification:\n",
      "   - Application de Naive Bayes, Multinomial Naive Bayes et Logistic Regression.\n",
      "   - Évaluation de la précision et comparaison des matrices de confusion.\n",
      "\n",
      "Partie 5: Classification Non Supervisée\n",
      "\n",
      "1. K-means:\n",
      "   - Application de l'algorithme K-means pour regrouper les produits en fonction de leurs caractéristiques textuelles.\n",
      "   - Évaluation avec le score de silhouette et le score NMI.\n",
      "\n",
      "Partie 6: Traitement des Données Visuelles\n",
      "\n",
      "1. Prétraitement des images:\n",
      "   - Redimensionnement, normalisation, conversion en niveaux de gris et égalisation de l'histogramme.\n",
      "\n",
      "2. Extraction de caractéristiques:\n",
      "   - Utilisation de SIFT et VGG16 pour extraire des caractéristiques des images prétraitées.\n",
      "\n",
      "3. Réduction de dimension avec PCA et t-SNE:\n",
      "   - Application de PCA pour réduire la dimensionnalité des caractéristiques extraites.\n",
      "   - Visualisation avec t-SNE pour observer la séparation des classes d'images.\n",
      "\n",
      "Partie 7: Classification Supervisée des Images\n",
      "\n",
      "1. Construction d'un CNN:\n",
      "   - Création d'un réseau de neurones convolutif pour la classification des images.\n",
      "   - Entraînement du modèle et évaluation de la précision sur l'ensemble de test.\n",
      "\n",
      "2. Enregistrement du modèle:\n",
      "   - Sauvegarde du modèle entraîné pour une utilisation future.\n",
      "\n",
      "Partie 8: API\n",
      "\n",
      "1. Utilisation d'une API externe:\n",
      "   - Requête à l'API Edamam pour obtenir des informations sur les produits alimentaires.\n",
      "   - Extraction et stockage des données dans un DataFrame.\n",
      "\n",
      "Conclusion:\n",
      "Ce projet a démontré l'application de diverses techniques de traitement de texte et d'image pour analyser et classifier des données e-commerce. Les modèles de machine learning ont permis de catégoriser les produits avec une précision significative, et les visualisations ont offert des insights sur la structure des données.\n"
     ]
    }
   ],
   "source": [
    "api=\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Import des librairies\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "# part 1\n",
    "\n",
    "# Observation et exploration du jeux de données\n",
    "\n",
    "\n",
    "df=pd.read_csv('/content/drive/MyDrive/OpenClassrooms/projet6/Dataset+projet+prétraitement+textes+images/Flipkart/flipkart_com-ecommerce_sample_1050.csv')\n",
    "df.head(2)\n",
    "\n",
    "df.info()\n",
    "\n",
    "df['product_category_tree']\n",
    "\n",
    "#la variable \"product_category_tree\" contient par ordre décroissant les catégroies que  peut avoir un article. cette catégorie va etre remplcer par trois varibles représentant les 2 premieres catégorie possible que peut avoir un article\n",
    "\n",
    "#splitting categories en cat1, cat2, cat3\n",
    "\n",
    "df['categorie 1']=df['product_category_tree'].apply(lambda x: x.split('[\"')[1].split('>>')[0].strip())\n",
    "df['categorie 2']=df['product_category_tree'].apply(lambda x: x.split('>>')[1].strip())\n",
    "df['categorie 3']=df['product_category_tree'].apply(lambda x: x.split('>>')[2].strip()if len(x.split('>>')) > 2 else \"None\")\n",
    "\n",
    "df=df.drop(columns=\"product_category_tree\")\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "df['product_specifications']\n",
    "\n",
    "# Analyse textuelle\n",
    "\n",
    "#L'analyse textuelle et l'extraction des features vont etre éffectuées par différentes approches mais seule l'analyse avec TFIDF va etre considérée pour la suite de projet\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stopwords_punct = set(punctuation)\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "all_stop_words=list(stop_words.union(stopwords_punct))\n",
    "\n",
    "class LemmaTokenizer:\n",
    "     def __init__(self):\n",
    "         self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "## TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf= TfidfVectorizer(lowercase=True, stop_words=all_stop_words,  tokenizer=LemmaTokenizer())\n",
    "\n",
    "tfidf_matrix=tfidf.fit_transform(df['description'])\n",
    "\n",
    "tfidf_feature_names=tfidf.get_feature_names_out() #why?:\n",
    "\n",
    "#tfidf_matrix = tfidf.transform(df['description'])\n",
    "\n",
    "#tfidf_matrix=tfidf_matrix.todense()[0,:].tolist()\n",
    "\n",
    "## Bag of words BOW\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(lowercase=True, stop_words=all_stop_words,  tokenizer=LemmaTokenizer())\n",
    "\n",
    "bow.fit_transform(df['description'])\n",
    "bow_feature_names=bow.get_feature_names_out()\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokens_word2vec= [list(tokenize(description, deacc=True, lower=True)) for description in df['description']]\n",
    "model_word2vec = Word2Vec(tokens_word2vec, vector_size=100, window=5, min_count=1, sg=0) #sg=0 pour CBOW\n",
    "\n",
    "model_word2vec.corpus_count\n",
    "\n",
    "vocab_word2vec = list(model_word2vec.wv.index_to_key)\n",
    "list(vocab_word2vec)[:5]\n",
    "\n",
    "## sentence embedding avec BERT\n",
    "\n",
    "pip install transformers torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "model_name = \"bert-base-uncased\"  # Exemple : BERT de base en minuscules\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "# Boucle sur les descriptions dans le DataFrame\n",
    "for description in df['description']:\n",
    "    # Tokenizez chaque texte individuellement\n",
    "    inputs = tokenizer(description, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Extraction des caractéristiques\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Ajoutez les embeddings du texte à la liste\n",
    "    embeddings.append(hidden_states)\n",
    "\n",
    "embeddings\n",
    "\n",
    "## sentence embedding avec USE\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Charger le modèle Universal Sentence Encoder\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Encodage des phrases en vecteurs\n",
    "sentence_embeddings = use_model(df['description'])\n",
    "\n",
    "sentence_embeddings\n",
    "\n",
    "# Réduction de dimensions avec NMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "nmf1 = NMF(n_components = df['categorie 1'].nunique(), random_state=1)\n",
    "nmf1.fit(tfidf_matrix)\n",
    "\n",
    "nmf1.get_params()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "tfidf_feature_names = tfidf.get_feature_names_out()\n",
    "print_top_words(nmf1, tfidf_feature_names, n_top_words = 10)\n",
    "\n",
    "nmf_data1=df[['description','categorie 1']]\n",
    "nmf_data1.head(2)\n",
    "\n",
    "val_best_topic1 = {}\n",
    "best_topic1 = {}\n",
    "matrix1 = nmf1.transform(tfidf_matrix)\n",
    "for i in range(0,matrix1.shape[0]):\n",
    "    val_best_topic1[i] = 0\n",
    "    for j in range(0, matrix1.shape[1]):\n",
    "        if matrix1[i,j] > val_best_topic1[i]:\n",
    "            val_best_topic1[i] = matrix1[i,j]\n",
    "            best_topic1[i]= j\n",
    "\n",
    "nmf_data1['topic_nmf'] = list(best_topic1.values())\n",
    "\n",
    "nmf_data1[['topic_nmf', 'categorie 1']].groupby(['topic_nmf', 'categorie 1']).nunique()#.sort_values(by=['topic_nmf', 'categorie 1'],ascending=[True, False])\n",
    "\n",
    "# Réduction de dimension supplémentaire avec t-SNE pour la visualisation 2D\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(matrix1)\n",
    "# Affichage graphique\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
    "plt.title('Visualisation t-SNE des données réduites avec NMF')\n",
    "plt.xlabel('Composant 1')\n",
    "plt.ylabel('Composant 2')\n",
    "plt.show()\n",
    "\n",
    "nmf2 = NMF(n_components = df['categorie 2'].nunique(), random_state=1)\n",
    "nmf2.fit(tfidf_matrix)\n",
    "\n",
    "print_top_words(nmf2, tfidf_feature_names, n_top_words = 10)\n",
    "\n",
    "nmf_data2=df[['description','categorie 1', 'categorie 2']]\n",
    "\n",
    "val_best_topic2 = {}\n",
    "best_topic2 = {}\n",
    "matrix2 = nmf2.transform(tfidf_matrix)\n",
    "for i in range(0,matrix2.shape[0]):\n",
    "    val_best_topic2[i] = 0\n",
    "    for j in range(0, matrix2.shape[1]):\n",
    "        if matrix2[i,j] > val_best_topic2[i]:\n",
    "            val_best_topic2[i] = matrix2[i,j]\n",
    "            best_topic2[i]= j\n",
    "\n",
    "\n",
    "nmf_data2['topic_nmf'] = list(best_topic2.values())\n",
    "\n",
    "nmf_data2[['topic_nmf', 'categorie 1', 'categorie 2']].groupby(['topic_nmf', 'categorie 2']).nunique().sort_values(by=['topic_nmf', 'categorie 1'],ascending=[True, False])\n",
    "\n",
    "# Réduction de dimension supplémentaire avec t-SNE pour la visualisation 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(matrix2)\n",
    "\n",
    "# Affichage graphique\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
    "plt.title('Visualisation t-SNE des données réduites avec NMF')\n",
    "plt.xlabel('Composant 1')\n",
    "plt.ylabel('Composant 2')\n",
    "plt.show()\n",
    "\n",
    "# Reduction de dimensions avec LDA\n",
    "\n",
    "from sklearn.decomposition import  LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = df['categorie 1'].nunique(),\n",
    "                               random_state=2)\n",
    "\n",
    "lda.fit_transform(tfidf_matrix)\n",
    "\n",
    "tfidf_feature_names = tfidf.get_feature_names_out()\n",
    "print_top_words(lda, tfidf_feature_names, n_top_words = 10)\n",
    "\n",
    "# classification supervisée\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "LE=LabelEncoder()\n",
    "y=LE.fit_transform(df['categorie 1'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)\n",
    "train_data = [(dict(enumerate(text.toarray()[0])), category) for text, category in zip(X_train, y_train)]\n",
    "test_data = [(dict(enumerate(text.toarray()[0])), category) for text, category in zip(X_test, y_test)]\n",
    "\n",
    "tfidf_matrix\n",
    "\n",
    "X_train[0]\n",
    "\n",
    "## Classification Naive Bayes\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "nb_classifier = NaiveBayesClassifier.train(train_data)\n",
    "nb_predictions = [nb_classifier.classify(features) for features, _ in test_data]\n",
    "\n",
    "nb_accuracy = nltk.classify.accuracy(nb_classifier, test_data)\n",
    "\n",
    "nb_confusion = ([category for _, category in test_data], nb_predictions) #confusion_matrix(nb_classifier, test_data)\n",
    "\n",
    "print(\"Accuracy:\", nb_accuracy)\n",
    "print(\"Naive Bayes confusion matrix:\", nb_confusion)\n",
    "\n",
    "\n",
    "\n",
    "# les features les plus représentatives des classes\n",
    "nb_classifier.show_most_informative_features(n=25)\n",
    "\n",
    "## Classification multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier=MultinomialNB()\n",
    "mnb_classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "mnb_accuracy=accuracy_score(mnb_classifier.predict(X_test), y_test)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", mnb_accuracy)\n",
    "\n",
    "mnb_confusion = confusion_matrix(mnb_classifier.predict(X_test), y_test)\n",
    "print(\"Multinomial Naive Bayes confusion matrix:\", mnb_confusion)\n",
    "\n",
    "## Classifier Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, logistic_regression.predict(X_test))\n",
    "\n",
    "print(\"Logistic regression accuracy:\", lr_accuracy)\n",
    "\n",
    "lr_confusion = confusion_matrix(y_test, logistic_regression.predict(X_test))\n",
    "print(\"Logistic regression confusion matrix:\", lr_confusion)\n",
    "\n",
    "#comparaison entre les 3 classifieurs\n",
    "\n",
    "scores = pd.DataFrame({'Model': ['Naive Bayes', 'Multinomial Naive Bayes', 'logistic regression'],\n",
    "                       'Accuracy': [nb_accuracy, mnb_accuracy, lr_accuracy]})\n",
    "scores\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Définissez la taille du graphique\n",
    "plt.bar(scores['Model'], scores['Accuracy'], color=['blue', 'green'])  # Créez le graphique à barres\n",
    "plt.xlabel('Model')  # Étiquette de l'axe x\n",
    "plt.ylabel('accuracy')  # Étiquette de l'axe y\n",
    "plt.title('Accuracy des modeles de classification')  # Titre du graphique\n",
    "plt.ylim(0, 1)  # Définissez la plage de l'axe y entre 0 et 1 (l'exactitude est généralement comprise entre 0 et 1)\n",
    "plt.show()\n",
    "\n",
    "# Classification non supervisée\n",
    "\n",
    "## K-means\n",
    "\n",
    "\n",
    "#Kmeans clustering selon la somme d'achat\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 7, init = 'k-means++', random_state = 42)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "#plotting\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "y_true = y#df['categorie 1']\n",
    "y_pred = cluster_labels\n",
    "\n",
    "# Calculez le score NMI\n",
    "nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "\n",
    "print(f\"NMI Score: {nmi}\")\n",
    "\n",
    "#Extraction des features images\n",
    "\n",
    "## Pré-traitement des données visuelles\n",
    "\n",
    "\n",
    "#prétraitement des données visuelles\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "dossier_images =('/content/drive/MyDrive/OpenClassrooms/projet6/Dataset+projet+prétraitement+textes+images/Flipkart/Images/')\n",
    "df['image_path']=dossier_images+df['image']\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "from PIL import Image as Image_PIL\n",
    "\n",
    "min_x = 100000\n",
    "max_x = 0\n",
    "min_y = 100000\n",
    "max_y = 0\n",
    "for image in df['image_path']:\n",
    "    shape = np.array(Image_PIL.open(image)).shape\n",
    "    if shape[0] < min_x:\n",
    "        min_x = shape[0]\n",
    "    elif shape[0] > max_x:\n",
    "        max_x = shape[0]\n",
    "    if shape[1] < min_y:\n",
    "        min_y = shape[1]\n",
    "    elif shape[1] > max_y:\n",
    "        max_y = shape[1]\n",
    "\n",
    "\n",
    "\n",
    "print('min_x {},\\nmax_x {},\\nmin_y {},\\nmax_y {}'.format(min_x, max_x, min_y, max_y))\n",
    "\n",
    "# Charger chaque image et effectuer le prétraitement\n",
    "chemins_images_pretraitees = []\n",
    "repertoire_destination = '/content/drive/MyDrive/OpenClassrooms/projet6/Dataset+projet+prétraitement+textes+images/Flipkart/images_preraitees/'\n",
    "\n",
    "for image_path in df['image_path']:\n",
    "    image = cv2.imread(image_path)\n",
    "    #redimension de l'image\n",
    "    image_redimensionnee = cv2.resize(image, (300, 300))\n",
    "    #normalisation des pixels, why?:\n",
    "    image_normalisee = image_redimensionnee.astype(float) / 255.0\n",
    "    image_normalisee_8u = cv2.normalize(image_normalisee, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    #conversion de l'image en gris\n",
    "    image_gris = cv2.cvtColor(image_normalisee_8u, cv2.COLOR_BGR2GRAY)\n",
    "    # Égalisation de l'histogramme\n",
    "    image_egale_hist = cv2.equalizeHist(image_gris)\n",
    "    #réduction de bruit\n",
    "    image_lisse = cv2.GaussianBlur(image_gris, (5, 5), 0)\n",
    "\n",
    "    # Enregistrez l'image prétraitée dans le répertoire de destination\n",
    "    nom_fichier = image_path.split('/')[-1]\n",
    "    chemin_destination = os.path.join(repertoire_destination, nom_fichier)\n",
    "    cv2.imwrite(chemin_destination, image_lisse)\n",
    "\n",
    "    # Ajoutez le chemin de l'image prétraitée à la liste\n",
    "    chemins_images_pretraitees.append(chemin_destination)\n",
    "\n",
    "# Ajoutez la liste des chemins des images prétraitées à votre DataFrame\n",
    "df['image_pretraite'] = chemins_images_pretraitees\n",
    "\n",
    "image_data = pd.concat([df['image_pretraite'], df['categorie 1']], axis=1)\n",
    "\n",
    "image_data.head(2)\n",
    "\n",
    "image_data.to_csv('/content/drive/MyDrive/OpenClassrooms/projet6/data_saved/image_data.csv', index=False)\n",
    "\n",
    "## Extraction des features avec SIFT\n",
    "\n",
    "import cv2\n",
    "# Initialiser le détecteur SURF\n",
    "surf = cv2.SIFT_create()\n",
    "features_sift = []\n",
    "\n",
    "for image_path in image_data['image_pretraite']:\n",
    "\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "\n",
    "    # Détecter les points d'intérêt et calculer les descripteurs\n",
    "    keypoints, descriptors = surf.detectAndCompute(image, None)\n",
    "\n",
    "    # Dessiner les points d'intérêt sur l'image (optionnel)\n",
    "    keypoints, descriptors = surf.detectAndCompute(image, None)\n",
    "\n",
    "    # Enregistrez les features\n",
    "    features_sift.append(descriptors)\n",
    "\n",
    "## Extraction des features avec VGG16\n",
    "\n",
    "X = []\n",
    "for image_path in image_data['image_pretraite']:\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Charger l'image en niveaux de gris\n",
    "    image = cv2.resize(image, (64, 64))  # Redimensionner l'image à la forme attendue (64x64)\n",
    "    #image = image / 255.0  # Normaliser les valeurs des pixels à [0, 1]\n",
    "    X.append(image)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = np.array(X)\n",
    "y=image_data['categorie 1']\n",
    "\n",
    "LE=LabelEncoder()\n",
    "y = LE.fit_transform(np.array(y).reshape(-1, 1))\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Charger le modèle CNN pré-entraîné (VGG16 dans cet exemple)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "# Les images sont en niveaux de gris, nous devons les convertir en images RVB en dupliquant le canal\n",
    "X_rgb = np.stack((X,) * 3, axis=-1)\n",
    "\n",
    "# Prétraitement des images et extraction des caractéristiques à partir du modèle\n",
    "preprocessed_images = preprocess_input(X_rgb)\n",
    "features = base_model.predict(preprocessed_images)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "features.shape\n",
    "\n",
    "## PCA\n",
    "\n",
    "from sklearn import manifold, decomposition\n",
    "\n",
    "print(features.shape)\n",
    "\n",
    "flattened_features = features.reshape(features.shape[0], -1)\n",
    "print(flattened_features.shape)\n",
    "\n",
    "pca = decomposition.PCA(n_components=0.99)\n",
    "feat_pca= pca.fit_transform(flattened_features)\n",
    "print(feat_pca.shape)\n",
    "\n",
    "#Réduction de dimension T-SNE et affichage des images selon vraies classes\n",
    "from sklearn import manifold, decomposition\n",
    "import time\n",
    "\n",
    "temps1 = time.time()\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000, init='random', random_state=6)\n",
    "X_tsne = tsne.fit_transform(feat_pca)\n",
    "\n",
    "duration1=time.time()-temps1\n",
    "print(\"temps de T-SNE : \", \"%15.2f\" % duration1, \"secondes\")\n",
    "\n",
    "df_tsne = pd.DataFrame(X_tsne, columns=['tsne1', 'tsne2'])\n",
    "df_tsne[\"class\"] = image_data['categorie 1']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne1\", y=\"tsne2\",\n",
    "    hue=\"class\",\n",
    "    palette=sns.color_palette('tab10', n_colors=4), s=50, alpha=0.6,\n",
    "    data=df_tsne,\n",
    "    legend=\"brief\")\n",
    "\n",
    "plt.title('TSNE selon les vraies classes', fontsize = 30, pad = 35, fontweight = 'bold')\n",
    "plt.xlabel('tsne1', fontsize = 26, fontweight = 'bold')\n",
    "plt.ylabel('tsne2', fontsize = 26, fontweight = 'bold')\n",
    "plt.legend(prop={'size': 14})\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Partie 2\n",
    "\n",
    "## classification supervisée\n",
    "\n",
    "classification supervisée des images en cnn!!\n",
    "\n",
    "\n",
    "image_data=pd.read_csv('/content/drive/MyDrive/OpenClassrooms/projet6/data_saved/image_data.csv')\n",
    "\n",
    "X = []\n",
    "for image_path in image_data['image_pretraite']:\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Charger l'image en niveaux de gris\n",
    "    image = cv2.resize(image, (64, 64))  # Redimensionner l'image à la forme attendue (64x64)\n",
    "    #image = image / 255.0  # Normaliser les valeurs des pixels à [0, 1]\n",
    "    X.append(image)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = np.array(X)\n",
    "y=image_data['categorie 1']\n",
    "\n",
    "LE=LabelEncoder()\n",
    "y = LE.fit_transform(np.array(y).reshape(-1, 1))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#data augmentation, pivoter les images , netteté, ...\n",
    "\n",
    "#Initialising the CNN\n",
    "cnn = tf.keras.models.Sequential()\n",
    "# Step 1 - Convolution\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(4,4), activation='relu', input_shape=(64, 64, 1)))\n",
    "# Step 2 - Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "#Adding a second convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(2,2), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=1))\n",
    "\n",
    "#Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#full connection\n",
    "cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "#output layer\n",
    "cnn.add(tf.keras.layers.Dense(len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "#compiling\n",
    "\n",
    "cnn.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "#training and test\n",
    "cnn.fit(x=X_train, y=y_train, epochs=25, batch_size=30, verbose=1)\n",
    "\n",
    "test_loss, test_acc = cnn.evaluate(X_test, y_test)\n",
    "print('Précision sur l\\'ensemble de test :', test_acc)\n",
    "print('Perte sur l\\'ensemble de test :', test_loss)\n",
    "\n",
    "for couche in cnn.get_weights():\n",
    "    print(couche.shape)\n",
    "\n",
    "y_pred=np.argmax(cnn.predict(X_test), axis=1)\n",
    "y_pred\n",
    "\n",
    "y_test\n",
    "\n",
    "#enregistrement du model sous HDF5\n",
    "cnn.save('/content/drive/MyDrive/OpenClassrooms/projet6/cnn_islem.h5')\n",
    "\n",
    "## API\n",
    "\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from IPython import display\n",
    "\n",
    "url = \"https://edamam-food-and-grocery-database.p.rapidapi.com/api/food-database/v2/parser\"\n",
    "\n",
    "querystring = \"ingr=champagne\"\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": \"7383f43809mshbfc6494ad959d9ep1a847ejsn564d551e39a3\",\n",
    "\t\"X-RapidAPI-Host\": \"edamam-food-and-grocery-database.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers , params=querystring)\n",
    "contenu=response.json()\n",
    "pprint(contenu)\n",
    "\n",
    "response.text\n",
    "\n",
    "print(type(contenu))\n",
    "\n",
    "contenu.keys()\n",
    "\n",
    "produits = contenu['hints'][:10]\n",
    "\n",
    "# Créer une liste pour stocker les données des produits\n",
    "donnees_produits = []\n",
    "\n",
    "# Parcourir les produits et extraire les informations requises\n",
    "for produit in produits:\n",
    "    food_id = produit['food']['foodId']\n",
    "    label = produit['food']['label']\n",
    "    category = produit['food']['category']\n",
    "    food_contents_label = produit['food']['foodContentsLabel'] if 'foodContentsLabel' in produit['food'] else np.nan\n",
    "    image = produit['food']['image'] if 'image' in produit['food'] else np.nan\n",
    "\n",
    "\n",
    "    # Ajouter les données du produit à la liste\n",
    "    donnees_produits.append({\n",
    "        'foodId': food_id,\n",
    "        'label': label,\n",
    "        'category': category,\n",
    "        'foodContentsLabel': food_contents_label,\n",
    "        'image': image\n",
    "    })\n",
    "\n",
    "produit_data=pd.DataFrame(donnees_produits)\n",
    "produit_data.to_csv('/content/drive/MyDrive/OpenClassrooms/projet6/Dataset+projet+prétraitement+textes+images/Flipkart/Habibi_Islem_4_produits_082023.csv', index=False)\n",
    "\"\"\"\n",
    "\n",
    "prompt=f\"\"\"\n",
    "rédige le contenue d'une presentation/rapport qui décrit en détail le travail réalisé par le code: {api}\"\"\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b2abfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour afficher la figure Plotly de la même manière que le graphique SHAP, vous devez convertir le graphique Plotly en une image codée en base64 et l'envoyer via JSON, tout comme vous l'avez fait pour le graphique SHAP. Voici comment vous pouvez le faire :\n",
      "\n",
      "1. Générez le graphique Plotly et convertissez-le en une image au format PNG.\n",
      "2. Encodez l'image en base64.\n",
      "3. Envoyez l'image codée en base64 dans la réponse JSON.\n",
      "\n",
      "Voici le code corrigé avec les modifications nécessaires pour afficher la figure Plotly :\n",
      "\n",
      "```python\n",
      "# Importations supplémentaires nécessaires\n",
      "from plotly.io import to_image\n",
      "\n",
      "# ...\n",
      "\n",
      "@app.route('/predict_proba', methods=['POST'])\n",
      "def predict_proba():\n",
      "    if request.method == 'POST':\n",
      "        try:\n",
      "            # ... (le reste du code reste inchangé)\n",
      "\n",
      "            #plotly graph\n",
      "            ref_df = pd.read_csv('training_data.csv')\n",
      "            # ...\n",
      "\n",
      "            # Convertir le graphique Plotly en image PNG\n",
      "            plotly_img_bytes = to_image(fig, format='png')\n",
      "\n",
      "            # Encoder l'image en base64\n",
      "            plotly_img_base64 = base64.b64encode(plotly_img_bytes).decode()\n",
      "\n",
      "            # ...\n",
      "\n",
      "            # Préparation de la réponse JSON\n",
      "            response = {'prediction': prediction, 'plotly_plot': 'data:image/png;base64,' + plotly_img_base64}\n",
      "\n",
      "            # Si la prédiction est supérieure à 50, ajoutez le graphique SHAP\n",
      "            if prediction[0] > 50:  # Assurez-vous que prediction est une liste et que vous comparez le premier élément\n",
      "                explainer = shap.TreeExplainer(model)\n",
      "                shap_values = explainer.shap_values(df)\n",
      "\n",
      "                # ...\n",
      "\n",
      "                response['shap_plot'] = 'data:image/png;base64,' + plot_data\n",
      "\n",
      "            return jsonify(response)\n",
      "        except Exception as e:\n",
      "            return jsonify({'error': str(e)})\n",
      "\n",
      "# ...\n",
      "```\n",
      "\n",
      "Notez que j'ai ajouté une clé 'plotly_plot' à l'objet de réponse JSON pour contenir l'image Plotly codée en base64. J'ai également corrigé la condition `if prediction > 50:` pour s'assurer qu'elle compare le premier élément de la liste `prediction` avec 50, car `prediction` est une liste de valeurs.\n",
      "\n",
      "Assurez-vous que le fichier 'training_data.csv' est présent et accessible par votre application Flask pour éviter toute erreur de fichier introuvable.\n",
      "\n",
      "Enfin, pour afficher ces images sur le côté client (par exemple, dans un navigateur web), vous devrez écrire du code HTML/JavaScript qui récupère la réponse JSON et insère les images codées en base64 dans la source d'une balise `<img>`.\n"
     ]
    }
   ],
   "source": [
    "code=\"\"\"\n",
    "import shap\n",
    "from flask import Flask, jsonify, request, render_template\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "# Charger le modèle pickle\n",
    "model = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def roundVal(n):\n",
    "    return (n * 100).round(2)\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def home():\n",
    "    return render_template('home.html')\n",
    "\n",
    "@app.route('/predict_proba', methods=['POST'])\n",
    "def predict_proba():\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            data_df = request.get_json(force=True)\n",
    "            #print(f\"shape data df data: {data_df[\"data\"].shape]})\n",
    "            \n",
    "                       \n",
    "            data = list(map(float, data_df[\"data\"]))\n",
    "            data[-1]=int(data[-1])\n",
    "            print(f\"data est: \\n: {data}\")\n",
    "\n",
    "            df = pd.DataFrame([data], columns=data_df[\"keys\"])\n",
    "            if 'SK_ID_CURR' in df.columns:\n",
    "\n",
    "                df=df.drop('SK_ID_CURR', axis=1)\n",
    "            print(f'df est:\\n {df}')\n",
    "\n",
    "            prediction = model.predict_proba(df)[:, 1]\n",
    "            print(prediction)\n",
    "            prediction = list(map(roundVal, prediction))\n",
    "            print(prediction)\n",
    "\n",
    "            #plotly graph\n",
    "            ref_df=pd.read_csv('training_data.csv')\n",
    "            print(\"ref_df\")\n",
    "\n",
    "            # Création d'un histogramme pour la distribution des revenus du client\n",
    "            fig = px.histogram(df, x='AMT_INCOME_TOTAL', nbins=50, title='Distribution des Revenus du Client')\n",
    "\n",
    "            \n",
    "            # Ajout d'un histogramme pour la distribution des revenus de référence\n",
    "            fig.add_histogram(x=ref_df['AMT_INCOME_TOTAL'], nbins=50, opacity=0.5, name='Revenus de Référence')\n",
    "\n",
    "            # Mise en page du graphique\n",
    "            fig.update_layout(\n",
    "                xaxis_title='Revenus',\n",
    "                yaxis_title='Fréquence',\n",
    "                legend_title='Distribution',\n",
    "            )\n",
    "            print(\"fig en cours\")\n",
    "\n",
    "            # Affichage du graphique\n",
    "            fig.show()\n",
    "\n",
    "            print(\"fig plotly faite\")\n",
    "            \n",
    "            if prediction>50:\n",
    "        \n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(df)\n",
    "\n",
    "                plt.figure()\n",
    "                shap.force_plot(explainer.expected_value, shap_values[0], df, matplotlib=True)\n",
    "                plt.tight_layout()\n",
    "                img = io.BytesIO()\n",
    "                plt.savefig(img, format='png')\n",
    "                plt.close()\n",
    "                img.seek(0)\n",
    "                plot_data = base64.b64encode(img.getvalue()).decode()\n",
    "\n",
    "            return jsonify({'prediction': prediction, 'shap_plot': 'data:image/png;base64,' + plot_data})\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)})\n",
    "\n",
    "@app.route('/predict_proba', methods=['GET'])\n",
    "def predict_proba_get():\n",
    "    return render_template('index.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=9000, debug=True)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "prompt=f\"corrige le code: {code} pour afficher la figure plotly de la meme maniere que le graph de shap\"\n",
    "\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv\"sample_test.csv\"\n",
    "\n",
    "\n",
    "# Création d'un histogramme pour la distribution des revenus du client\n",
    "            fig = px.histogram(df, x='AMT_INCOME_TOTAL', nbins=50, title='Distribution des Revenus du Client')\n",
    "\n",
    "            # Ajout d'un histogramme pour la distribution des revenus de référence\n",
    "            fig.add_histogram(x=ref_df['AMT_INCOME_TOTAL'], nbins=50, opacity=0.5, name='Revenus de Référence')\n",
    "\n",
    "            # Mise en page du graphique\n",
    "            fig.update_layout(\n",
    "                xaxis_title='Revenus',\n",
    "                yaxis_title='Fréquence',\n",
    "                legend_title='Distribution',\n",
    "            )\n",
    "\n",
    "            # Convert Plotly figure to HTML for embedding\n",
    "            plotly_fig_html = fig.to_html(full_html=False)\n",
    "\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(df)\n",
    "\n",
    "            shap_html = shap.force_plot(explainer.expected_value, shap_values[0], df, show=False, matplotlib=False)\n",
    "            shap_html = shap.save_html(\"shap_plot.html\", shap_html)\n",
    "\n",
    "            with open(\"shap_plot.html\", \"r\") as f:\n",
    "                shap_html_data = f.read()\n",
    "\n",
    "            return jsonify({'prediction': prediction, 'plotly_fig': plotly_fig_html, 'shap_plot': shap_html_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec3150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
